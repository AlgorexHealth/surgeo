{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pathlib\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background (links good as of 2019-12-10)\n",
    "\n",
    "<h2 style=\"color: red;\">This takes a fair amount of RAM and download time</h2>\n",
    "\n",
    "This script fetches ~50 .sf1 state and territory zip files from the census server at the following location:\n",
    "\n",
    "* https://www2.census.gov/census_2010/04-Summary_File_1/\n",
    "\n",
    "These each of these zip files contains (among other things) a geographic file and a population file containing.\n",
    "\n",
    "The fixed-width geo file is useful because it ties a  \"Logical Record\" to ZCTA codes. The CSV population file is useful because it contains the actual data we need for the logical records.\n",
    "\n",
    "Specifically, we are pulling our data from population table 9 (\"P9\" ... see Summary File 1 documentation page  184).\n",
    "\n",
    "    P5. HISPANIC OR LATINO ORIGIN BY RACE Universe: Total population (17)\n",
    "\n",
    "The data for this table is as follows:\n",
    "\n",
    "    P5 data:\n",
    "        Total:\n",
    "            Not Hispanic or Latino:\n",
    "                White alone\n",
    "                Black or African American alone\n",
    "                American Indian and Alaska Native alone\n",
    "                Asian alone\n",
    "                Native Hawaiian and Other Pacific Islander alone \n",
    "                Some Other Race alone\n",
    "                Two or More Races\n",
    "            Hispanic or Latino:\n",
    "                White alone\n",
    "                Black or African American alone \n",
    "                American Indian and Alaska Native alone\n",
    "                Asian alone\n",
    "                Native Hawaiian and Other Pacific Islander alone \n",
    "                Some Other Race alone\n",
    "                Two or More Races\n",
    "\n",
    "## Notes:\n",
    "* Geo table must be filtered by summary level == 871 (State-5-Digit ZIP Code Tabulation Area) so that it is limited to ZCTA records.\n",
    "* The cut down geo table is joined to the population table using the LOGRECNO (Log Record Number)\n",
    "* All Hispanic groups are condense to a single group to make consistent with surname data\n",
    "* Asian and API are combined to make consistent with surname data\n",
    "* \"Some Other Race\" is apporitoned abong the groups.\n",
    "\n",
    "For more details, the technical documentation for Summary File 1 can be found here:\n",
    "* https://www.census.gov/prod/cen2010/doc/sf1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the start/stop indices for the fixed width geo file.\n",
    "# It allows them to be easily converted to dataframes.\n",
    "GEO_MAP_2010 = {\n",
    "    'FILEID'  : (1  , 7  ),\n",
    "    'STUSAB'  : (7  , 9  ),\n",
    "    'SUMLEV'  : (9  , 12 ),\n",
    "    'GEOCOMP' : (12 , 14 ),\n",
    "    'CHARITER': (14 , 17 ),\n",
    "    'CIFSN'   : (17 , 19 ),\n",
    "    'LOGRECNO': (19 , 26 ),\n",
    "    'REGION'  : (26 , 27 ),\n",
    "    'DIVISION': (27 , 28 ),\n",
    "    'STATE'   : (28 , 30 ),\n",
    "    'COUNTY'  : (30 , 33 ),\n",
    "    'COUNTYCC': (33 , 35 ),\n",
    "    'COUNTYSC': (35 , 37 ),\n",
    "    'COUSUB'  : (37 , 42 ),\n",
    "    'COUSUBCC': (42 , 44 ),\n",
    "    'COUSUBSC': (44 , 46 ),\n",
    "    'PLACE'   : (46 , 51 ),\n",
    "    'PLACECC' : (51 , 53 ),\n",
    "    'PLACESC' : (53 , 55 ),\n",
    "    'TRACT'   : (55 , 61 ),\n",
    "    'BLKGRP'  : (61 , 62 ),\n",
    "    'BLOCK'   : (62 , 66 ),\n",
    "    'IUC'     : (66 , 68 ),\n",
    "    'CONCIT'  : (68 , 73 ),\n",
    "    'CONCITCC': (73 , 75 ),\n",
    "    'CONCITSC': (75 , 77 ),\n",
    "    'AIANHH'  : (77 , 81 ),\n",
    "    'AIANHHFP': (81 , 86 ),\n",
    "    'AIANHHCC': (86 , 88 ),\n",
    "    'AIHHTLI' : (88 , 89 ),\n",
    "    'AITSCE'  : (89 , 92 ),\n",
    "    'AITS'    : (92 , 97 ),\n",
    "    'AITSCC'  : (97 , 99 ),\n",
    "    'TTRACT'  : (99 , 105),\n",
    "    'TBLKGRP' : (105, 106),\n",
    "    'ANRC'    : (106, 111),\n",
    "    'ANRCCC'  : (111, 113),\n",
    "    'CBSA'    : (113, 118),\n",
    "    'CBSASC'  : (118, 120),\n",
    "    'METDIV'  : (120, 125),\n",
    "    'CSA'     : (125, 128),\n",
    "    'NECTA'   : (128, 133),\n",
    "    'NECTASC' : (133, 135),\n",
    "    'NECTADIV': (135, 140),\n",
    "    'CNECTA'  : (140, 143),\n",
    "    'CBSAPCI' : (143, 144),\n",
    "    'NECTAPCI': (144, 145),\n",
    "    'UA'      : (145, 150),\n",
    "    'UASC'    : (150, 152),\n",
    "    'UATYPE'  : (152, 153),\n",
    "    'UR'      : (153, 154),\n",
    "    'CD'      : (154, 156),\n",
    "    'SLDU'    : (156, 159),\n",
    "    'SLDL'    : (159, 162),\n",
    "    'VTD'     : (162, 168),\n",
    "    'VTDI'    : (168, 169),\n",
    "    'RESERVE2': (169, 172),\n",
    "    'ZCTA5'   : (172, 177),\n",
    "    'SUBMCD'  : (177, 182),\n",
    "    'SUBMCDCC': (182, 184),\n",
    "    'SDELM'   : (184, 189),\n",
    "    'SDSEC'   : (189, 194),\n",
    "    'SDUNI'   : (194, 199),\n",
    "    'AREALAND': (119, 213),\n",
    "    'AREAWATR': (213, 227),\n",
    "    'NAME'    : (227, 317),\n",
    "    'FUNCSTAT': (317, 318),\n",
    "    'GCUNI'   : (318, 319),\n",
    "    'POP100'  : (319, 328),\n",
    "    'HU100'   : (328, 337),\n",
    "    'INTPTLAT': (337, 348),\n",
    "    'INTPTLON': (348, 360),\n",
    "    'LSADC'   : (360, 362),\n",
    "    'PARTFLAG': (362, 363),\n",
    "    'RESERVE3': (363, 369),\n",
    "    'UGA'     : (369, 374),\n",
    "    'STATENS' : (374, 382),\n",
    "    'COUNTYNS': (382, 390),\n",
    "    'COUSUBNS': (390, 398),\n",
    "    'PLACENS' : (398, 406),\n",
    "    'CONCITNS': (406, 414),\n",
    "    'AIANHHNS': (414, 422),\n",
    "    'AITSNS'  : (422, 430),\n",
    "    'ANRCNS'  : (430, 438),\n",
    "    'SUBMCDNS': (438, 446),\n",
    "    'CD113'   : (446, 448),\n",
    "    'CD114'   : (448, 450),\n",
    "    'CD115'   : (450, 452),\n",
    "    'SLDU2'   : (452, 455),\n",
    "    'SLDU3'   : (455, 458),\n",
    "    'SLDU4'   : (458, 461),\n",
    "    'SLDL2'   : (461, 464),\n",
    "    'SLDL3'   : (464, 467),\n",
    "    'SLDL4'   : (467, 470),\n",
    "    'AIANHHSC': (470, 472),\n",
    "    'CSASC'   : (472, 476),\n",
    "    'CNECTASC': (474, 477),\n",
    "    'MEMI'    : (476, 478),\n",
    "    'NMEMI'   : (477, 478),\n",
    "    'PUMA'    : (478, 483),\n",
    "    'RESERVED': (483, 501),\n",
    "}\n",
    "\n",
    "# Our zip downloads have URLs that can be recreated with state/arrevs.\n",
    "STATES = {\n",
    "    'AL': 'Alabama',\n",
    "    'AK': 'Alaska',\n",
    "    'AZ': 'Arizona',\n",
    "    'AR': 'Arkansas',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DE': 'Delaware',\n",
    "    'DC': 'District_of_Columbia',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'IA': 'Iowa',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MS': 'Mississippi',\n",
    "    'MO': 'Missouri',\n",
    "    'MT': 'Montana',\n",
    "    'NE': 'Nebraska',\n",
    "    'NV': 'Nevada',\n",
    "    'NH': 'New_Hampshire',\n",
    "    'NJ': 'New_Jersey',\n",
    "    'NM': 'New_Mexico',\n",
    "    'NY': 'New_York',\n",
    "    'NC': 'North_Carolina',\n",
    "    'ND': 'North_Dakota',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'PR': 'Puerto_Rico',\n",
    "    'RI': 'Rhode_Island',\n",
    "    'SC': 'South_Carolina',\n",
    "    'SD': 'South_Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VT': 'Vermont',\n",
    "    'VA': 'Virginia',\n",
    "    'WA': 'Washington',\n",
    "    'WV': 'West_Virginia',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WY': 'Wyoming',    \n",
    "}\n",
    "\n",
    "# This is the template for the URLs\n",
    "URL_TEMPLATE_ZIP = 'https://www2.census.gov/census_2010/04-Summary_File_1/{state}/{state_abbrev}2010.sf1.zip'\n",
    "\n",
    "# This creates a URL with each and every state in the STATES dictioanry\n",
    "urls = [\n",
    "    URL_TEMPLATE_ZIP.format(state_abbrev=code.lower(), state=name)\n",
    "    for code, name\n",
    "    in STATES.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_file(url):\n",
    "    '''Helper func: downloads zip from URL and stores it in file-like object'''\n",
    "    # Open request\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        # Read into bytes IO and return\n",
    "        data = io.BytesIO(f.read())\n",
    "        return data\n",
    "\n",
    "def make_geo_df(data):\n",
    "    '''Helper func: takes zip and creates a geographic file from data'''\n",
    "    with zipfile.ZipFile(data) as zf:\n",
    "        # Filter out everything except the ZipInfo (geo) for csv we want\n",
    "        target = zf.filelist[0]\n",
    "        # Read that CSV into BytesIO object\n",
    "        geo_data = io.BytesIO(zf.read(target))\n",
    "        # Read fixed-width file into dataframe\n",
    "        geo_df = pd.read_fwf(\n",
    "            geo_data, \n",
    "            header=None,\n",
    "            # Use the GEO MAP but subtract from column start/stop\n",
    "            colspecs=[\n",
    "                (tuple_[0] - 1, tuple_[1] - 1)\n",
    "                for tuple_\n",
    "                in GEO_MAP_2010.values()\n",
    "            ],\n",
    "            dtype=str\n",
    "        )\n",
    "    # Give names to columns\n",
    "    geo_df.columns = tuple(GEO_MAP_2010.keys())\n",
    "    # Filter out all records that are not related to ZCTAs only\n",
    "    # e.g. get rid of census block data\n",
    "    geo_df = geo_df.loc[geo_df.SUMLEV == '871']\n",
    "    # Keep the STUSAB (state), LOGRECONO (join key), and ZCTA5 (zip code proxy)\n",
    "    geo_df = geo_df[['STUSAB', 'LOGRECNO', 'ZCTA5']]#.dropna(subset=['ZCTA5'])\n",
    "    return geo_df\n",
    "\n",
    "def make_pop_df(data):\n",
    "    '''Helper func: Takes a zip and creates population df'''\n",
    "    with zipfile.ZipFile(data) as zf:\n",
    "        # Filter out everything except the ZipInfo for csv we want\n",
    "        # This contains Table P5\n",
    "        target = zf.filelist[3]\n",
    "        # Read that CSV into BytesIO object\n",
    "        pop_data = io.BytesIO(zf.read(target))\n",
    "        pop_df = pd.read_csv(\n",
    "            pop_data, \n",
    "            header=None,\n",
    "            dtype=str\n",
    "        )\n",
    "        # Keep only a subset of columns and renames them\n",
    "        pop_df = pop_df[[1, 4, 18, 19, 20, 21, 22, 23, 24, 25]]\n",
    "        pop_df.columns = [\n",
    "            'STUSAB',\n",
    "            'LOGRECNO',\n",
    "            'white',\n",
    "            'black',\n",
    "            'native',\n",
    "            'asian',\n",
    "            'pi',\n",
    "            'other',\n",
    "            'multiple',\n",
    "            'hispanic',\n",
    "        ]\n",
    "        return pop_df\n",
    "\n",
    "def merge_frames(geo_df, pop_df):\n",
    "    '''Merges our GEO and POP frames'''\n",
    "    # Merges common STUSAB and LOGRECNO fields\n",
    "    merged = geo_df.merge(pop_df)\n",
    "    # Rename zctq5\n",
    "    merged = merged.rename(columns={'ZCTA5': 'zcta5'})\n",
    "    # Set index to ZCTA5 and sort\n",
    "    merged = merged.set_index('zcta5')\n",
    "    merged = merged.sort_index()\n",
    "    return merged\n",
    "    \n",
    "def create_df(url):\n",
    "    '''Main function to download, join, and clean data for single state'''\n",
    "    print(url)\n",
    "    # Download\n",
    "    data   = dl_file(url)\n",
    "    # Make dfs\n",
    "    geo_df = make_geo_df(data)\n",
    "    pop_df = make_pop_df(data)\n",
    "    # Join DFs, sort, trip, and process\n",
    "    df     = merge_frames(geo_df, pop_df)\n",
    "    df = df.iloc[:, 2:]\n",
    "    df = df.astype(np.float64)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download.\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Alabama/al2010.sf1.zip\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d03c20068551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcreate_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d03c20068551>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m data = [\n\u001b[0;32m      5\u001b[0m     \u001b[0mcreate_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m ]\n",
      "\u001b[1;32m<ipython-input-3-82ed09acc000>\u001b[0m in \u001b[0;36mcreate_df\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mdata\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mdl_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Make dfs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mgeo_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_geo_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mpop_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pop_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;31m# Join DFs, sort, trip, and process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-82ed09acc000>\u001b[0m in \u001b[0;36mmake_geo_df\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_geo_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;34m'''Helper func: takes zip and creates a geographic file from data'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Filter out everything except the ZipInfo (geo) for csv we want\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilelist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1222\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1223\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m                 \u001b[1;31m# set the modified flag so central directory gets written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1289\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "print('Starting download.')\n",
    "\n",
    "# Create a dataframe for each URL and store in list\n",
    "data = [\n",
    "    create_df(url)\n",
    "    for url\n",
    "    in urls\n",
    "]\n",
    "\n",
    "print('Download complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all data into single dataframe and sort index\n",
    "df = pd.concat(data)\n",
    "df = df.sort_index()\n",
    "\n",
    "# https://github.com/theonaunheim/surgeo/issues/10\n",
    "# Certain zctas cross state lines and must be added together.\n",
    "df = df.groupby(df.index).apply(sum)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store column totals\n",
    "totals = df.sum(axis=1)\n",
    "totals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store some other race so it can be divvyed up among other groups\n",
    "other = df['other']\n",
    "other.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Asian or Pacific Islander (this is what surname uses)\n",
    "df['api'] = df['asian'] + df['pi']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will no longer use\n",
    "df = df.drop(columns=['other', 'asian', 'pi'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now determine what percent of the row each items makes up.\n",
    "percentages = df.divide(totals, axis='rows')\n",
    "percentages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up 'other' into the remaining rows based on the percents above\n",
    "apportioned_other = percentages.multiply(other, axis='rows')\n",
    "apportioned_other.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute 'other' to the remaining groups based on percentage makeup\n",
    "# quasi Iterative proportortional fit / matrix rake over single dimension\n",
    "df += apportioned_other\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconvert to percentage\n",
    "column_totals = df.sum(axis=0)\n",
    "ratio_by_column = df.divide(column_totals, axis='columns').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "ratio_by_column = ratio_by_column[[\n",
    "    'white',\n",
    "    'black',\n",
    "    'api',\n",
    "    'native',\n",
    "    'multiple',\n",
    "    'hispanic'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconvert to percentage\n",
    "row_totals = df.sum(axis=1)\n",
    "ratio_by_row = df.divide(row_totals, axis='index').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "ratio_by_row = ratio_by_row[[\n",
    "    'white',\n",
    "    'black',\n",
    "    'api',\n",
    "    'native',\n",
    "    'multiple',\n",
    "    'hispanic'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write data to module as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = pathlib.Path().cwd()\n",
    "project_directory = current_directory.parents[0]\n",
    "data_directory    = project_directory / 'surgeo' / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob zcta given race\n",
    "rbc_path = data_directory / 'prob_zcta_given_race_2010.csv'\n",
    "ratio_by_column.to_csv(rbc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob race given block zcta\n",
    "rbr_path = data_directory / 'prob_race_given_zcta_2010.csv'\n",
    "ratio_by_row.to_csv(rbr_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
